# Summarization module

**Problem** 

Automatic testing based on symbolic engine can produce a tremendous amount of unit
tests for an average sized code project.

There are so many tests sometimes that it is difficult to understand how one test differs from another. 
For tests based on symbolic execution, we can try to explain to the user which statements are covered 
by this test, as opposed to its neighbor. For tests generated by the fuzzing platform, 
we can focus on the key features of the input parameters, their types and values, 
as well as facilitate navigation in the huge number of generated tests by structuring them into groups 
in a certain way.

**Solution**

The summarization process includes the generation of the following meta-information:
- method names for tests
- display names for tests
- JavaDocs for tests
- simple comments for group of tests (regions)

If the summarization process for some reason was failed at one of the stages due to an error or insufficient information,
then the test method receives a unique name and no more meta-information.

The approach to generating meta-information depends on the type of UtExection for which the meta-information is being created and can vary significantly.
Also, JavaDocs built in two modes: as plain text or in especial format enriched with the custom java tags.

This subsystem is fully located in the ```utbot-summary``` module.

**What does the framework get as input?**

In the process of test generation, as the last stage, 
the ```UtMethodTestSet.summarize``` method is called, 
which enriches the individual ```UtExecutions``` contained in ```UtMethodTestSet``` with meta-information, 
or rather, with method names, display names and comments, 
and also groups the received ```UtExecutions``` into clusters and generates names for these clusters (e.g. region names).

As a result, the set of ```UtExecutions``` with empty fields ```testMethodName```, ```displayName```, ```summary``` filled 
with nulls is an input of the Summarization framework, 
and the output is the same set with successfully filled fields mentioned earlier.

```UtExecutions``` are not homogeneous objects and can be generated from different sources, 
as well as have a different set of data based on which meta-information for tests will be generated.

At the time of this writing, there are three main types of ```UtExecutions```: ```UtSymbolicExecution```, ```UtFailedExecution```, 
and ```UtFuzzedExecution```.

If method parameters, their values and types, as well as the return type are used to construct meta-information for ```UtFuzzedExecutions```, 
then information about the results of symbolic code analysis is used to solve a similar task for ```UtSymbolicExecutions```.

**The basic process for constructing meta information for UtSymbolicExecutions**

1. Java symbolic engine generates a set of unit tests for each method under test. 
2. Each unit test traverses a particular path in the method under test. 
3. The engine’s API accepts Java bytecode, then transforms it to Soot’s Jimple instructions
4. API of symbolic engine also provides access to each such execution path presented as ```List<Step>``` (the sequence of Jimple statements)
where each execution ```Step``` consists of statement, depth and decision.
   - Statement or stmt - Soot's Jimple statements. Symbolic engines uses simplified representation of bytecode commands which is Jimple statements for analytical traversal of different execution path.
   - Depth of execution step – depicts an execution depth of the statement in call graph where method under test is a root.
   - Decision of execution step – a number which describes the direction of execution inside the control flow graph. 
   If there are two outcoming edges from execution statement in the control flow graph, decision number shows which next edge is chosen to be executed.
5. Then, for each pair of UtTestCase and its source code file, it analyzes the popular code constructs encountered, such as unique steps, recursions, iteration cycles, iterations passed, etc.
6. As a result of this analysis, tags or meta-tags are generated. Tags represent execution in structural view with the result of analysis and are further used to directly create meta-information.
7. The Summarization process includes the following sub-steps:
   - parsing the source file that contains method under test using JavaParser.
   - mapping source code to statements (result: each Jimple statement is mapped to AST node)
   - building the Sentence Blocks (building bricks for Custom tags or plain mode)
   - building the Final Sentence (is valid for plain mode only)
   - generation the display names and method names based on the following simple rule: in each execution path, it finds the last unique statement,
     preferably condition statement that uniquely executed (satisfied or unsatisfied).
     Next, AST node of this statement is used for naming the execution.
   - building the cluster names based on the common execution path

If you want to know more about difference between plain mode and Custom JavaDoc tags mode, 
please refer to the [CustomJavadocTags user guide](https://github.com/UnitTestBot/UTBotJava/blob/main/docs/summaries/CustomJavadocTags.md).

**Tagging process**

In the steps 6-7 from the previous chapter the tagging process is described. Let's give some insights about that process.

It includes the generation of different tags bounded to the specific code lines.

At this moment, we could assign the following set of tags:

- Unique, Common, Partly Common
- Decision in Control Flow Graph -> Right, Left, Return, etc.
- Number of statement executions in a given test
- First call, second call, etc.
- Type: statement starts iteration, ends iteration, invokes the recursion, etc.

We use our own implementation of the DBSCAN clustering algorithm with the non-euclidean distance measure 
based on the widely known Minimum Edit Distance to identify unique, common and partly common execution steps. 

Firstly, execution paths manually divided into groups: 
- successfully executed paths (only this group is clustered into different classes with DBSCAN)
- paths with expected throw exceptions
- paths with unexpected throw exceptions
- other groups with errors and exceptions based on the given ```UtResult```

In each cluster, we identify unique, common and partly common execution steps. 
- Basically, the execution step is _unique_ if no other execution path in the cluster contains this step so only one execution triggers this statement in its cluster. 
- _Partly_ common means that only part of executions in cluster contains this step. 
- _Common_ statements - all paths executes these statements.

**The basic process for constructing meta information for UtFuzzedExecutions**

For ```UtFuzzedExecution```, meta-information is also available in the form:
- names for test methods
- display names
- JavaDocs (contains only navigation links)
- region names

At the same time, it is worth noting that tests obtained using fuzzing are clustered only based on the received ```UtResult```, 
without additional division into subgroups for all successfully completed tests, 
unlike tests obtained from ```UtSymbolicExecutions```.

The meta-information generation algorithm is contained in the ```ModelBasedNameSuggester```, 
which in turn can be the registration point for smaller and simpler approaches that implement the ```SingleModelNameSuggester``` interface, 
such as ```PrimitiveModelNameSuggester``` and ```ArrayModelNameSuggester```.

Depending on the received ```UtExecutionResult``` type, the basis of the method name or the display of the name is formed. 
Then information about the types, names and values of the parameters of the method under test is attached to this base. 
This information is retrieved from the ```FuzzedMethodDescription``` type values contained in the ```UtFuzzedExecution``` and the ```FuzzedValue``` list.

NOTE: It is important to note that in case of a large number of method parameters (more than three), 
test names and display names will not be generated.



